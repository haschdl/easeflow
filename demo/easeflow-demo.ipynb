{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“˜ Easing Functions & Perlin Noise\n",
    "Easing functions control how values transition over time, making animations and data curves **smooth and natural**. Perlin noise adds **realistic variations**, commonly used in **graphics, simulations, and time-series generation**.\n",
    "\n",
    "### ðŸ”¹ What Are Easing Functions?\n",
    "Instead of moving at a constant speed, easing functions create **acceleration and deceleration effects**. Examples:\n",
    "- **CubicEaseInOut** â†’ Smooth start & end  \n",
    "- **QuadEaseIn** â†’ Slow start, fast finish  \n",
    "- **CircularEaseIn** â†’ Simulates circular motion  \n",
    "\n",
    "### ðŸ”¹ Why Use Perlin Noise?\n",
    "Perlin noise introduces **fluid randomness**, making transitions **less artificial**. Itâ€™s great for **terrain generation, animations, and synthetic data**.\n",
    "\n",
    "### ðŸ“Œ What This Notebook Covers\n",
    "âœ… Visualizing easing functions (**Matplotlib**)  \n",
    "âœ… Adding Perlin noise for realistic variation  \n",
    "âœ… Generating synthetic data with **`easeflow`** (PySpark)  \n",
    "\n",
    "ðŸ‘‰ **Run the next cells to explore these concepts interactively!** ðŸš€  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Easing Functions & Perlin Noise (Matplotlib)\n",
    "\n",
    " > Skip this section if you are familiar with both Easing functions and Perlin noise. \n",
    "\n",
    "Before diving in the simplified approach the `easeflow` provided, let's check a simple example using Matplot lib and numpy.\n",
    "\n",
    "In the next cell, we will visualize various easing functions and see how Perlin noise can be applied to them. This will help us understand the impact of noise on smooth transitions and how it can make animations and synthetic data more dynamic and realistic.\n",
    "\n",
    "We will use interactive widgets to adjust the noise parameters and observe the changes in real-time. This interactive approach allows for a deeper exploration of how different easing functions behave under the influence of Perlin noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0a4cf6226204295b6f9f0063f5ab57b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.3, description='Noise Max', max=5.0), FloatSlider(value=2.0, descripâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from ipywidgets import interact, FloatSlider\n",
    "from perlin_noise import PerlinNoise\n",
    "import easing_functions as easy\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "# Global PerlinNoise object\n",
    "noise = PerlinNoise(octaves=10, seed=1)\n",
    "\n",
    "\n",
    "def apply_noise(easing: callable, x: np.ndarray, noise_max: float, noise_speed: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Applies Perlin noise to a given easing function to create more natural variation.\n",
    "\n",
    "    Easing functions produce smooth curves that can represent animations, synthetic data,\n",
    "    or interpolation. By adding Perlin noise, we introduce **controlled randomness** \n",
    "    while keeping the general shape of the easing function.\n",
    "\n",
    "    Args:\n",
    "        easing (callable): The easing function (e.g., CubicEaseInOut).\n",
    "        x (np.ndarray): A normalized array of values from 0 to 1.\n",
    "        noise_max (float): The maximum noise influence (0 = no noise).\n",
    "        noise_speed (float): Controls noise frequency (higher = more variation).\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The noisy easing function values.\n",
    "    \"\"\"\n",
    "    noise_func = np.vectorize(lambda val: val * (1 - noise_max + noise_max * (1 + noise(val * noise_speed))))\n",
    "    return noise_func(easing(x))\n",
    "\n",
    "\n",
    "def plot_subplot(ax: plt.Axes, easing_function: callable, x: np.ndarray, noise_max: float, noise_speed: float) -> None:\n",
    "    \"\"\"\n",
    "    Plots an easing function with and without Perlin noise.\n",
    "\n",
    "    This helps visualize how Perlin noise modifies smooth transitions, making \n",
    "    easing functions more dynamic and realistic.\n",
    "\n",
    "    Args:\n",
    "        ax (plt.Axes): The subplot axis for visualization.\n",
    "        easing_function (callable): The easing function (e.g., QuinticEaseIn).\n",
    "        x (np.ndarray): A normalized array of values from 0 to 1.\n",
    "        noise_max (float): The maximum noise influence.\n",
    "        noise_speed (float): Frequency of the noise variation.\n",
    "    \"\"\"\n",
    "    easing = np.vectorize(easing_function(start=0.0, end=1.0))\n",
    "    \n",
    "    # Compute smooth and noisy curves\n",
    "    y_smooth = easing(x)\n",
    "    y_noisy = apply_noise(easing, x, noise_max, noise_speed)\n",
    "\n",
    "    ax.set_title(easing_function.__name__, fontsize=10)\n",
    "    ax.plot(x, y_smooth, label=\"No Noise\", linewidth=2)\n",
    "    ax.plot(x, y_noisy, label=\"With Perlin Noise\", linestyle=\"solid\", alpha=0.8)\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "\n",
    "\n",
    "def plot_examples(noise_max: float = 0.3, noise_speed: float = 0.001) -> None:\n",
    "    \"\"\"\n",
    "    Plots multiple easing functions with and without Perlin noise.\n",
    "\n",
    "    This example is useful for **understanding** how easing functions work \n",
    "    and how Perlin noise modifies them.\n",
    "\n",
    "    Args:\n",
    "        noise_max (float): Maximum noise influence (default: 0.3).\n",
    "        noise_speed (float): Frequency of noise variation (default: 0.001).\n",
    "    \"\"\"\n",
    "    functions = [easy.CubicEaseInOut, easy.QuadEaseIn, easy.QuinticEaseIn, easy.CircularEaseIn]\n",
    "    \n",
    "    num_functions = len(functions)\n",
    "    num_cols = 2  # Fixed number of columns for better visualization\n",
    "    num_rows = math.ceil(num_functions / num_cols)\n",
    "\n",
    "    fig, axs = plt.subplots(num_rows, num_cols, figsize=(10, 6), sharex=True, sharey=True)\n",
    "    fig.suptitle(\"Examples of Easing Functions with Noise Applied\", fontsize=12)\n",
    "\n",
    "    x_values = np.linspace(0, 1, 1000)  # High-resolution curve\n",
    "\n",
    "    for i, easing_function in enumerate(functions):\n",
    "        row, col = divmod(i, num_cols)\n",
    "        plot_subplot(axs[row, col], easing_function, x_values, noise_max, noise_speed)\n",
    "\n",
    "    # Remove empty subplots if an odd number of functions\n",
    "    if num_functions % num_cols != 0:\n",
    "        fig.delaxes(axs[-1, -1])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def show_interactive():\n",
    "    \"\"\"\n",
    "    Interactive widget for adjusting noise parameters and visualizing easing functions.\n",
    "\n",
    "    This allows users to explore how Perlin noise affects different easing functions.\n",
    "    \"\"\"\n",
    "    noise_max_slider = FloatSlider(value=0.3, min=0.0, max=5.0, step=0.1, description=\"Noise Max\")\n",
    "    noise_speed_slider = FloatSlider(value=2, min=0.0, max=5.0, step=0.01, description=\"Noise Speed\")\n",
    "    \n",
    "    interact(plot_examples, noise_max=noise_max_slider, noise_speed=noise_speed_slider)\n",
    "\n",
    "show_interactive()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 - Using `easeflow` for Synthetic Data Generation (Spark)\n",
    "\n",
    "`easeflow` provides two key functions for synthetic data generation:\n",
    "\n",
    "- **`norm_df(n: int)`** â†’ Returns a DataFrame with:\n",
    "  - `id` â†’ Sequential index (0 to `n-1`)\n",
    "  - `t` â†’ Normalized values (0 to 1)\n",
    "\n",
    "- **`make_udf(easing_function, min_val, max_val)`** â†’ Creates a **PySpark UDF** that applies an easing function, optionally modified with Perlin noise.\n",
    "\n",
    "### ðŸ”¹ Generating a Normalized DataFrame\n",
    "To create a structured dataset for easing functions:\n",
    "```python\n",
    "df = norm_df(365)\n",
    "display(df)\n",
    "```\n",
    "\n",
    "Output:  \n",
    "```\n",
    "+---+------------------+  \n",
    "| id|                 t|  \n",
    "+---+------------------+  \n",
    "|  0|  0.0             |  \n",
    "|  1|  0.00274         |  \n",
    "|  2|  0.00548         |  \n",
    "|...|  ...             |  \n",
    "|364|  1.0             |  \n",
    "+---+------------------+  \n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”¹ Applying an Easing Function with Noise  \n",
    "\n",
    "We can now use make_udf to apply an easing function to this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('../')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24b51579269a4ac984ce8d120b2441c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, bar_style='success'), Label(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/databricks/spark/python/pyspark/serializers.py\", line 192, in _read_with_length\n    return self.loads(obj)\n           ^^^^^^^^^^^^^^^\n  File \"/databricks/spark/python/pyspark/serializers.py\", line 609, in loads\n    return cloudpickle.loads(obj, encoding=encoding)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nModuleNotFoundError: No module named 'easing_functions'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/databricks/spark/python/pyspark/worker.py\", line 2029, in main\n    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n                                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/spark/python/pyspark/worker.py\", line 1916, in read_udfs\n    read_single_udf(\n  File \"/databricks/spark/python/pyspark/worker.py\", line 819, in read_single_udf\n    f, return_type = read_command(pickleSer, infile)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/spark/python/pyspark/worker_util.py\", line 71, in read_command\n    command = serializer._read_with_length(file)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/spark/python/pyspark/serializers.py\", line 196, in _read_with_length\n    raise SerializationError(\"Caused by \" + traceback.format_exc())\npyspark.serializers.SerializationError: Caused by Traceback (most recent call last):\n  File \"/databricks/spark/python/pyspark/serializers.py\", line 192, in _read_with_length\n    return self.loads(obj)\n           ^^^^^^^^^^^^^^^\n  File \"/databricks/spark/python/pyspark/serializers.py\", line 609, in loads\n    return cloudpickle.loads(obj, encoding=encoding)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nModuleNotFoundError: No module named 'easing_functions'\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPythonException\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Git/easeflow/.venv/lib/python3.12/site-packages/IPython/core/formatters.py:402\u001b[39m, in \u001b[36mBaseFormatter.__call__\u001b[39m\u001b[34m(self, obj)\u001b[39m\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m402\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprinter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[38;5;66;03m# Finally look for special method names\u001b[39;00m\n\u001b[32m    404\u001b[39m method = get_real_method(obj, \u001b[38;5;28mself\u001b[39m.print_method)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.ipython/profile_default/startup/00-databricks-init-cb19ae4000cfcff5929f2f6a411c35ea.py:366\u001b[39m, in \u001b[36mregister_formatters.<locals>.df_html\u001b[39m\u001b[34m(df)\u001b[39m\n\u001b[32m    365\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdf_html\u001b[39m(df):\n\u001b[32m--> \u001b[39m\u001b[32m366\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnotebook_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdataframe_display_limit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtoPandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.to_html()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Git/easeflow/.venv/lib/python3.12/site-packages/pyspark/sql/connect/dataframe.py:2000\u001b[39m, in \u001b[36mDataFrame.toPandas\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1998\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtoPandas\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[33m\"\u001b[39m\u001b[33mpandas.DataFrame\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1999\u001b[39m     query = \u001b[38;5;28mself\u001b[39m._plan.to_proto(\u001b[38;5;28mself\u001b[39m._session.client)\n\u001b[32m-> \u001b[39m\u001b[32m2000\u001b[39m     pdf, ei = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_session\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_pandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_plan\u001b[49m\u001b[43m.\u001b[49m\u001b[43mobservations\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2001\u001b[39m     \u001b[38;5;28mself\u001b[39m._execution_info = ei\n\u001b[32m   2002\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m pdf\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Git/easeflow/.venv/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:1244\u001b[39m, in \u001b[36mSparkConnectClient.to_pandas\u001b[39m\u001b[34m(self, plan, observations)\u001b[39m\n\u001b[32m   1240\u001b[39m (self_destruct_conf,) = \u001b[38;5;28mself\u001b[39m.get_config_with_defaults(\n\u001b[32m   1241\u001b[39m     (\u001b[33m\"\u001b[39m\u001b[33mspark.sql.execution.arrow.pyspark.selfDestruct.enabled\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mfalse\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   1242\u001b[39m )\n\u001b[32m   1243\u001b[39m self_destruct = cast(\u001b[38;5;28mstr\u001b[39m, self_destruct_conf).lower() == \u001b[33m\"\u001b[39m\u001b[33mtrue\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1244\u001b[39m table, schema, metrics, observed_metrics, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_execute_and_fetch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1245\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobservations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_destruct\u001b[49m\u001b[43m=\u001b[49m\u001b[43mself_destruct\u001b[49m\n\u001b[32m   1246\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1247\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m table \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1248\u001b[39m ei = ExecutionInfo(metrics, observed_metrics)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Git/easeflow/.venv/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:1919\u001b[39m, in \u001b[36mSparkConnectClient._execute_and_fetch\u001b[39m\u001b[34m(self, req, observations, extra_request_metadata, self_destruct)\u001b[39m\n\u001b[32m   1916\u001b[39m properties: Dict[\u001b[38;5;28mstr\u001b[39m, Any] = {}\n\u001b[32m   1918\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Progress(handlers=\u001b[38;5;28mself\u001b[39m._progress_handlers, operation_id=req.operation_id) \u001b[38;5;28;01mas\u001b[39;00m progress:\n\u001b[32m-> \u001b[39m\u001b[32m1919\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_execute_and_fetch_as_iterator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1920\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobservations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_request_metadata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress\u001b[49m\n\u001b[32m   1921\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1922\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mStructType\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1923\u001b[39m \u001b[43m            \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Git/easeflow/.venv/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:1895\u001b[39m, in \u001b[36mSparkConnectClient._execute_and_fetch_as_iterator\u001b[39m\u001b[34m(self, req, observations, extra_request_metadata, progress)\u001b[39m\n\u001b[32m   1893\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m kb\n\u001b[32m   1894\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[32m-> \u001b[39m\u001b[32m1895\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_handle_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Git/easeflow/.venv/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2211\u001b[39m, in \u001b[36mSparkConnectClient._handle_error\u001b[39m\u001b[34m(self, error)\u001b[39m\n\u001b[32m   2209\u001b[39m \u001b[38;5;28mself\u001b[39m.thread_local.inside_error_handling = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   2210\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(error, grpc.RpcError):\n\u001b[32m-> \u001b[39m\u001b[32m2211\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_handle_rpc_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2212\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(error, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[32m   2213\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mCannot invoke RPC\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(error) \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mclosed\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(error):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Git/easeflow/.venv/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2315\u001b[39m, in \u001b[36mSparkConnectClient._handle_rpc_error\u001b[39m\u001b[34m(self, rpc_error)\u001b[39m\n\u001b[32m   2300\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\n\u001b[32m   2301\u001b[39m                     \u001b[33m\"\u001b[39m\u001b[33mPython versions in the Spark Connect client and server are different. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2302\u001b[39m                     \u001b[33m\"\u001b[39m\u001b[33mTo execute user-defined functions, client and server should have the \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2311\u001b[39m                     \u001b[33m\"\u001b[39m\u001b[33mhttps://docs.databricks.com/en/release-notes/serverless.html.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2312\u001b[39m                 )\n\u001b[32m   2313\u001b[39m             \u001b[38;5;66;03m# END-EDGE\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2315\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m convert_exception(\n\u001b[32m   2316\u001b[39m                 info,\n\u001b[32m   2317\u001b[39m                 status.message,\n\u001b[32m   2318\u001b[39m                 \u001b[38;5;28mself\u001b[39m._fetch_enriched_error(info),\n\u001b[32m   2319\u001b[39m                 \u001b[38;5;28mself\u001b[39m._display_server_stack_trace(),\n\u001b[32m   2320\u001b[39m             ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2322\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m SparkConnectGrpcException(status.message) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2323\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mPythonException\u001b[39m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/databricks/spark/python/pyspark/serializers.py\", line 192, in _read_with_length\n    return self.loads(obj)\n           ^^^^^^^^^^^^^^^\n  File \"/databricks/spark/python/pyspark/serializers.py\", line 609, in loads\n    return cloudpickle.loads(obj, encoding=encoding)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nModuleNotFoundError: No module named 'easing_functions'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/databricks/spark/python/pyspark/worker.py\", line 2029, in main\n    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n                                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/spark/python/pyspark/worker.py\", line 1916, in read_udfs\n    read_single_udf(\n  File \"/databricks/spark/python/pyspark/worker.py\", line 819, in read_single_udf\n    f, return_type = read_command(pickleSer, infile)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/spark/python/pyspark/worker_util.py\", line 71, in read_command\n    command = serializer._read_with_length(file)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/spark/python/pyspark/serializers.py\", line 196, in _read_with_length\n    raise SerializationError(\"Caused by \" + traceback.format_exc())\npyspark.serializers.SerializationError: Caused by Traceback (most recent call last):\n  File \"/databricks/spark/python/pyspark/serializers.py\", line 192, in _read_with_length\n    return self.loads(obj)\n           ^^^^^^^^^^^^^^^\n  File \"/databricks/spark/python/pyspark/serializers.py\", line 609, in loads\n    return cloudpickle.loads(obj, encoding=encoding)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nModuleNotFoundError: No module named 'easing_functions'\n\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint, t: double, v: float, v_noise: float]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from easeflow import make_udf, norm_df, ease\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Create an easing function UDF\n",
    "easing_udf = make_udf(ease.QuinticEaseIn, min_val=500, max_val=1000)\n",
    "\n",
    "# Apply the easing function with and without noise\n",
    "df = (\n",
    "    norm_df(365)\n",
    "    .withColumn(\"v\", easing_udf(F.col(\"t\"), F.lit(0)))  # No noise\n",
    "    .withColumn(\"v_noise\", easing_udf(F.col(\"t\"), F.lit(0.3)))  # With Perlin noise\n",
    ")\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”¹ Adding Time-Series Data\n",
    "\n",
    "Since norm_df() provides an id column, we can map it to a date column to simulate a time-series dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Define start date\n",
    "dataset_len = 365\n",
    "start_date = datetime.today().replace(day=1) - timedelta(days=dataset_len)\n",
    "\n",
    "# Generate dataset with a date column\n",
    "df = (\n",
    "    norm_df(dataset_len)\n",
    "    .withColumn(\"date\", F.date_add(F.lit(start_date), F.col(\"id\").cast(\"integer\")))  # Convert `id` to a date\n",
    "    .withColumn(\"v\", easing_udf(F.col(\"t\"), F.lit(0)))  # No noise\n",
    "    .withColumn(\"v_noise\", easing_udf(F.col(\"t\"), F.lit(0.3)))  # With noise\n",
    ")\n",
    "\n",
    "display(df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
